{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-GIFIsKpZzs"
   },
   "source": [
    "# EV Charging Station Data Integration and Cleaning\n",
    "## Introduction\n",
    "**Electric vehicles (EVs)** are becoming increasingly popular worldwide, necessitating a robust network of charging stations. To facilitate data analysis and planning around EV infrastructure, it's essential to have accurate and comprehensive data on charging stations. This notebook demonstrates the process of acquiring, merging, and cleaning data on EV charging stations from two different sources:\n",
    "\n",
    "**OpenChargeMap API:** This API provides detailed information about EV charging stations globally, including station locations, connection types, operators, and more.\n",
    "\n",
    "**Overpass API (OpenStreetMap):** This API allows access to location-based data, including the locations of EV charging stations. It pulls data directly from OpenStreetMap, a community-driven mapping project.\n",
    "\n",
    "The objective of this notebook is to create a unified dataset that combines data from these two sources, cleans it, and prepares it for analysis. This dataset can then be used for various analytical purposes, such as mapping charger distribution, analyzing charger types and availability, and forecasting demand for charging infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlauhvEHpofe"
   },
   "source": [
    "## Data Acquisition\n",
    "\n",
    "In this section, we will fetch data from both the OpenChargeMap and Overpass APIs.\n",
    "\n",
    "\n",
    "*   OpenChargeMap API: This API provides detailed information about charging stations, including geographical location, connection types, and operator information. We use this API to get a comprehensive dataset of charging stations in Australia.\n",
    "*   Overpass API: OpenStreetMap (OSM) provides an extensive set of geographical data, including the location of EV charging stations. Using the Overpass API, we can query OSM for all nodes tagged as \"charging stations\" within Australia.\n",
    "\n",
    "\n",
    "\n",
    "These two APIs provide complementary data about EV charging stations. By combining them, we can fill in gaps and cross-verify the information for better accuracy and completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWqfHsjbps7Q"
   },
   "source": [
    "## Data Cleaning and Merging\n",
    "\n",
    "After acquiring data from both sources, we proceed with data cleaning and merging:\n",
    "\n",
    "1.   Cleaning OpenChargeMap Data: The data from the OpenChargeMap API includes a lot of metadata and redundant information that we don't need for our analysis. In this step, we drop irrelevant columns to focus only on the essential data, such as location, operator, connection type, and cost.\n",
    "2.   Cleaning Overpass Data: Similar to the OpenChargeMap data, the Overpass data contains extraneous details. We drop irrelevant columns and retain only the useful information, such as geographical coordinates and tags related to the charging station.\n",
    "1.   Merging Datasets: After cleaning both datasets, we merge them into a single DataFrame. This involves several steps:\n",
    "Column Renaming: To ensure consistency and avoid conflicts during the merge, we rename columns that represent the same information differently in each dataset (e.g., latitude and longitude).\n",
    "Unifying Data: We combine columns with similar data (e.g., operator information from both datasets) into unified columns.\n",
    "Handling Duplicates: We identify and handle duplicate entries based on location coordinates, preferring data from the OpenChargeMap where possible for consistency.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWONj3A-qUFz"
   },
   "source": [
    "## Data Processing and Transformation\n",
    "Once the data is merged, we process and transform it to ensure consistency and usability:\n",
    "\n",
    "Extracting and Normalizing Data: For example, we extract details from the 'connections' column, which contains nested information about the types of connections available at each charging station, such as connection type, power output, and current type.\n",
    "\n",
    "Handling Missing Values: We address missing data by either filling in default values or marking entries as 'Unknown' where appropriate. This ensures that our dataset is complete and consistent, even if some data is missing.\n",
    "\n",
    "Converting Data Types: We convert columns to appropriate data types (e.g., numerical, categorical) to facilitate easier analysis and ensure correct calculations during analysis.\n",
    "\n",
    "Final Data Cleaning: We further refine the dataset by dropping unnecessary columns and combining related information (e.g., merging brand and operator information into a single column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGqED3Ffql_K"
   },
   "source": [
    "## Final Dataset Preparation\n",
    "After all cleaning and processing steps, we finalize the dataset:\n",
    "\n",
    "\n",
    "*   Standardizing Columns: We rename columns for clarity and consistency (e.g., renaming 'NumberOfPoints' to 'ChargingPoints').\n",
    "*   Ensuring Data Completeness: We fill any remaining missing values, ensuring the dataset is ready for analysis.\n",
    "*   Adding Flags and Indicators: We add flags, such as a 'ChargingPoints_Flag' to indicate whether a charging station has zero points and convert it to one to ensure all entries have at least one charging point.\n",
    "\n",
    "\n",
    "The final dataset is a cleaned, unified, and comprehensive view of EV charging stations in Australia, ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Obtaining dependency information for geopy from https://files.pythonhosted.org/packages/e5/15/cf2a69ade4b194aa524ac75112d5caac37414b20a3a03e6865dfe0bd1539/geopy-2.4.1-py3-none-any.whl.metadata\n",
      "  Downloading geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting geographiclib<3,>=1.52 (from geopy)\n",
      "  Obtaining dependency information for geographiclib<3,>=1.52 from https://files.pythonhosted.org/packages/9f/5a/a26132406f1f40cf51ea349a5f11b0a46cec02a2031ff82e391c2537247a/geographiclib-2.0-py3-none-any.whl.metadata\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading geopy-2.4.1-py3-none-any.whl (125 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-2.0 geopy-2.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3A41QNq0Bmzz",
    "outputId": "34da5099-f43d-47fe-afcd-3bc588278b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data saved to cleaned_charging_stations.csv\n",
      "Number of records saved: 1712\n",
      "\n",
      "First few records of the saved data:\n",
      "      Cost  ChargingPoints PayAtLocation MembershipRequired AccessKeyRequired  \\\n",
      "0  Unknown               1       Unknown            Unknown           Unknown   \n",
      "2  Unknown               1       Unknown            Unknown           Unknown   \n",
      "3  Unknown               2       Unknown            Unknown           Unknown   \n",
      "4  Unknown               8       Unknown            Unknown           Unknown   \n",
      "8  Unknown               4       Unknown            Unknown           Unknown   \n",
      "\n",
      "  IsOperational   Latitude   Longitude            Operator  \\\n",
      "0       Unknown -31.936252  115.871531             Unknown   \n",
      "2       Unknown -32.935880  151.643848         ChargePoint   \n",
      "3       Unknown -41.218297  146.412523             Unknown   \n",
      "4       Unknown -34.755480  149.720994  Tesla Supercharger   \n",
      "8       Unknown -37.887228  145.082491         Tesla, Inc.   \n",
      "\n",
      "         ConnectionType  PowerOutput CurrentType  ChargingPoints_Flag  \n",
      "0               Unknown          NaN     Unknown                    1  \n",
      "2        Type 1 (J1772)          NaN     Unknown                    1  \n",
      "3  Type 2 (Socket Only)          NaN     Unknown                    0  \n",
      "4               Unknown          NaN     Unknown                    0  \n",
      "8               Unknown          NaN     Unknown                    0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:241: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_merged_df.drop(columns=['is_duplicate', 'origin', 'lat_round', 'lon_round'], inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[['ConnectionType', 'Power', 'CurrentType']] = df_cleaned['connections_ocm'].apply(parse_connections)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[['ConnectionType', 'Power', 'CurrentType']] = df_cleaned['connections_ocm'].apply(parse_connections)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[['ConnectionType', 'Power', 'CurrentType']] = df_cleaned['connections_ocm'].apply(parse_connections)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Power'] = pd.to_numeric(df_cleaned['Power'], errors='coerce')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:270: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['output'] = pd.to_numeric(df_cleaned['output'], errors='coerce')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:273: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Combined_Power_Output'] = df_cleaned.apply(\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:279: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned.drop(columns=columns_to_drop, inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:289: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['operator'] = df_cleaned.apply(combine_brand_operator, axis=1)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:292: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['capacity'] = pd.to_numeric(df_cleaned['capacity'], errors='coerce')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:293: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['NumberOfPoints'] = pd.to_numeric(df_cleaned['NumberOfPoints'], errors='coerce')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:296: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['NumberOfPoints'] = df_cleaned['NumberOfPoints'].fillna(0) + df_cleaned['capacity'].fillna(0)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:297: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned.drop(columns=['capacity'], inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned.drop(columns=columns_to_drop, inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:306: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['socket_type1'] = df_cleaned['socket_type1'].apply(lambda x: 'Type 1 (J1772)' if pd.notna(x) else x)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:307: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['socket_type2'] = df_cleaned['socket_type2'].apply(lambda x: 'Type 2 (Socket Only)' if pd.notna(x) else x)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['socket_chademo'] = df_cleaned['socket_chademo'].apply(lambda x: 'CHAdeMO' if pd.notna(x) else x)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:318: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['ConnectionType'] = df_cleaned.apply(merge_connection_types, axis=1)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:319: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned.drop(columns=['socket_type1', 'socket_type2', 'socket_chademo'], inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:322: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned.rename(columns={\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:338: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Cost'].fillna('Unknown', inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:339: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['CurrentType'].fillna('Unknown', inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:341: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['CurrentType'].fillna('Unknown', inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:342: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['PayAtLocation'].fillna('Unknown', inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:343: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['MembershipRequired'].fillna('Unknown', inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['AccessKeyRequired'].fillna('Unknown', inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:345: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['IsOperational'].fillna('Unknown', inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:346: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Operator'].fillna('Unknown', inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:347: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['ConnectionType'].fillna('Unknown', inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:350: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['PowerOutput'].fillna(np.nan, inplace=True)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:353: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['ChargingPoints_Flag'] = df_cleaned['ChargingPoints'].apply(lambda x: 1 if x == 0 else 0)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:354: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['ChargingPoints'] = df_cleaned['ChargingPoints'].apply(lambda x: 1 if x == 0 else x)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:357: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['ChargingPoints'] = df_cleaned['ChargingPoints'].astype(int)\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:358: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['IsOperational'] = df_cleaned['IsOperational'].astype('category')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:359: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['ConnectionType'] = df_cleaned['ConnectionType'].astype('category')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:360: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Operator'] = df_cleaned['Operator'].astype('category')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:361: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['CurrentType'] = df_cleaned['CurrentType'].astype('category')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Cost'] = df_cleaned['Cost'].astype('category')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:363: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['PowerOutput'] = pd.to_numeric(df_cleaned['PowerOutput'], errors='coerce')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:364: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['IsOperational'] = df_cleaned['IsOperational'].astype('category')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:365: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['ConnectionType'] = df_cleaned['ConnectionType'].astype('category')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:366: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Operator'] = df_cleaned['Operator'].astype('category')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:367: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['PayAtLocation'] = df_cleaned['PayAtLocation'].astype('category')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:368: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['MembershipRequired'] = df_cleaned['MembershipRequired'].astype('category')\n",
      "/var/folders/xc/kb91yk9x24942px3_wqcy2s00000gn/T/ipykernel_58328/241522816.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['AccessKeyRequired'] = df_cleaned['AccessKeyRequired'].astype('category')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import ast\n",
    "from geopy.distance import geodesic\n",
    "from scipy.spatial import KDTree\n",
    "import numpy as np\n",
    "\n",
    "def get_cleaned_charging_station_data(api_key):\n",
    "    # Fetch data from Open Charge Map API\n",
    "    def get_all_chargers_in_au(api_key):\n",
    "        url = \"https://api.openchargemap.io/v3/poi/\"\n",
    "        headers = {'X-API-Key': api_key}\n",
    "        params = {'countrycode': 'AU', 'maxresults': 10000}\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            response.raise_for_status()\n",
    "\n",
    "    # Normalize JSON data to DataFrame\n",
    "    def chargers_to_dataframe(charger_data):\n",
    "        df = pd.json_normalize(charger_data)\n",
    "        return df\n",
    "\n",
    "    # Fetch data from Overpass API\n",
    "    def get_osm_chargers():\n",
    "        overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "        overpass_query = \"\"\"\n",
    "        [out:json];\n",
    "        area[\"ISO3166-1\"=\"AU\"][admin_level=2];\n",
    "        node[\"amenity\"=\"charging_station\"](area);\n",
    "        out body;\n",
    "        \"\"\"\n",
    "        response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            elements = data['elements']\n",
    "            charging_stations = []\n",
    "            for element in elements:\n",
    "                lat = element.get('lat')\n",
    "                lon = element.get('lon')\n",
    "                tags = element.get('tags', {})\n",
    "                station_name = tags.get('name', 'Unknown')\n",
    "                charging_stations.append({\n",
    "                    'name': station_name,\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'tags': tags\n",
    "                })\n",
    "            return pd.DataFrame(charging_stations)\n",
    "        else:\n",
    "            raise Exception(f\"Error fetching OSM data: {response.status_code}\")\n",
    "\n",
    "    # Extract tags into separate columns\n",
    "    def extract_tags(df, column_name):\n",
    "        tags_df = pd.json_normalize(df[column_name])\n",
    "        df = pd.concat([df.drop(columns=[column_name]), tags_df], axis=1)\n",
    "        return df\n",
    "\n",
    "    # Unify common columns into single columns\n",
    "    def unify_columns(df, col1, col2, unified_col):\n",
    "        if col1 in df.columns and col2 in df.columns:\n",
    "            df[unified_col] = df[col1].combine_first(df[col2])\n",
    "            df.drop([col1, col2], axis=1, inplace=True)\n",
    "        elif col1 in df.columns:\n",
    "            df.rename(columns={col1: unified_col}, inplace=True)\n",
    "        elif col2 in df.columns:\n",
    "            df.rename(columns={col2: unified_col}, inplace=True)\n",
    "\n",
    "    # Parse connections\n",
    "    def parse_connections(connections):\n",
    "        connection_type = None\n",
    "        power = None\n",
    "        current_type = None\n",
    "\n",
    "        if isinstance(connections, str):\n",
    "            try:\n",
    "                connections = ast.literal_eval(connections)\n",
    "            except (ValueError, SyntaxError):\n",
    "                connections = []\n",
    "\n",
    "        if isinstance(connections, list):\n",
    "            for connection in connections:\n",
    "                if isinstance(connection, dict):\n",
    "                    conn_type = connection.get('ConnectionType', {})\n",
    "                    curr_type = connection.get('CurrentType', {})\n",
    "\n",
    "                    if isinstance(conn_type, dict) and connection_type is None:\n",
    "                        connection_type = conn_type.get('Title', 'Unknown')\n",
    "\n",
    "                    if power is None:\n",
    "                        power = connection.get('PowerKW', None)\n",
    "\n",
    "                    if isinstance(curr_type, dict) and current_type is None:\n",
    "                        current_type = curr_type.get('Title', 'Unknown')\n",
    "\n",
    "                    break\n",
    "\n",
    "        return pd.Series({'ConnectionType': connection_type, 'Power': power, 'CurrentType': current_type})\n",
    "\n",
    "    # Mark duplicates based on rounded coordinates, preferring df_ocm entries\n",
    "    def mark_prefer_ocm_duplicates(df):\n",
    "        df['is_duplicate'] = False\n",
    "        grouped = df.groupby(['lat_round', 'lon_round'])\n",
    "        for _, group in grouped:\n",
    "            if len(group) > 1:\n",
    "                ocm_index = group[group['origin'] == 'ocm'].index\n",
    "                if not ocm_index.empty:\n",
    "                    df.loc[group.index.difference(ocm_index), 'is_duplicate'] = True\n",
    "                else:\n",
    "                    df.loc[group.index[1:], 'is_duplicate'] = True\n",
    "\n",
    "    # Fetch OCM data\n",
    "    charger_data = get_all_chargers_in_au(api_key)\n",
    "    charger_df = chargers_to_dataframe(charger_data)\n",
    "\n",
    "    # Drop unnecessary columns from OCM data\n",
    "    columns_to_drop = [\n",
    "        'UserComments', 'PercentageSimilarity', 'MediaItems', 'ParentChargePointID',\n",
    "        'DataProvidersReference', 'OperatorsReference', 'GeneralComments', 'DatePlanned',\n",
    "        'DateLastConfirmed', 'MetadataValues', 'DataProvider.WebsiteURL',\n",
    "        'DataProvider.Comments', 'DataProvider.DataProviderStatusType.IsProviderEnabled',\n",
    "        'DataProvider.DataProviderStatusType.ID', 'DataProvider.DataProviderStatusType.Title',\n",
    "        'DataProvider.IsRestrictedEdit', 'DataProvider.IsOpenDataLicensed',\n",
    "        'DataProvider.IsApprovedImport', 'DataProvider.License',\n",
    "        'DataProvider.DateLastImported', 'DataProvider.ID', 'OperatorInfo.WebsiteURL',\n",
    "        'OperatorInfo.Comments', 'OperatorInfo.PhonePrimaryContact',\n",
    "        'OperatorInfo.PhoneSecondaryContact', 'OperatorInfo.IsPrivateIndividual',\n",
    "        'OperatorInfo.AddressInfo', 'OperatorInfo.BookingURL',\n",
    "        'OperatorInfo.ContactEmail', 'OperatorInfo.FaultReportEmail',\n",
    "        'OperatorInfo.IsRestrictedEdit', 'OperatorInfo.ID', 'OperatorInfo', 'UsageType'\n",
    "    ]\n",
    "    charger_df = charger_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # Fetch OSM data\n",
    "    df1 = get_osm_chargers()\n",
    "    df1 = extract_tags(df1, 'tags')\n",
    "\n",
    "    # Drop unnecessary columns from OSM data\n",
    "    columns_to_drop_osm = [\n",
    "        'opening_hours', 'brand:wikidata', 'operator:wikidata', 'brand:wikipedia',\n",
    "        'operator:wikipedia', 'description', 'name', 'short_name', 'source', 'location',\n",
    "        'addr:unit', 'website', 'note', 'ref:ocm', 'motorcar', 'bicycle', 'scooter',\n",
    "        'access:note', 'alt_name', 'temporary:access', 'source:operator', 'fixme', 'ref',\n",
    "        'maxstay', 'voltage', 'manufacturer', 'operator:website', 'no:network', 'bus',\n",
    "        'hgv', 'motorcycle', 'amperage', 'not:brand:wikidata', 'construction', 'ele',\n",
    "        'not:operator:wikidata', 'man_made', 'phone:AU', 'office', 'phone', 'start_date',\n",
    "        'wikidata', 'survey:date', 'parking', 'url', 'network:wikidata', 'check_date',\n",
    "        'indoor', 'branch'\n",
    "    ]\n",
    "    df1 = df1.drop(columns=columns_to_drop_osm, errors='ignore')\n",
    "\n",
    "    # Rename and merge columns\n",
    "    df_osm = df1.copy()\n",
    "    df_ocm = charger_df.copy()\n",
    "\n",
    "    df_osm['origin'] = 'osm'\n",
    "    df_ocm['origin'] = 'ocm'\n",
    "\n",
    "    common_columns_osm = {\n",
    "        'latitude': 'latitude_osm',\n",
    "        'longitude': 'longitude_osm',\n",
    "        'amenity': 'amenity_osm',\n",
    "        'brand': 'brand_osm',\n",
    "        'operator': 'operator_osm',\n",
    "        'capacity': 'capacity_osm',\n",
    "        'fee': 'fee_osm',\n",
    "        'charge': 'cost_osm',\n",
    "        'socket:type1': 'socket_type1_osm',\n",
    "        'socket:type2': 'socket_type2_osm',\n",
    "        'socket:chademo': 'socket_chademo_osm',\n",
    "        'addr:housenumber': 'address_house_number_osm',\n",
    "        'addr:street': 'address_street_osm',\n",
    "        'addr:city': 'address_city_osm',\n",
    "        'addr:country': 'address_country_osm',\n",
    "        'addr:postcode': 'address_postcode_osm',\n",
    "        'addr:state': 'address_state_osm',\n",
    "        'website': 'website_osm',\n",
    "        'network': 'network_osm'\n",
    "    }\n",
    "\n",
    "    common_columns_ocm = {\n",
    "        'AddressInfo.Latitude': 'latitude_ocm',\n",
    "        'AddressInfo.Longitude': 'longitude_ocm',\n",
    "        'Connections': 'connections_ocm',\n",
    "        'UsageCost': 'cost_ocm',\n",
    "        'OperatorInfo.Title': 'operator_ocm',\n",
    "        'AddressInfo.Title': 'address_title_ocm',\n",
    "        'AddressInfo.AddressLine1': 'address_line1_ocm',\n",
    "        'AddressInfo.AddressLine2': 'address_line2_ocm',\n",
    "        'AddressInfo.Town': 'address_city_ocm',\n",
    "        'AddressInfo.StateOrProvince': 'address_state_ocm',\n",
    "        'AddressInfo.Postcode': 'address_postcode_ocm',\n",
    "        'AddressInfo.Country.Title': 'address_country_ocm',\n",
    "        'AddressInfo.ContactTelephone1': 'contact_telephone1_ocm',\n",
    "        'AddressInfo.ContactTelephone2': 'contact_telephone2_ocm',\n",
    "        'AddressInfo.ContactEmail': 'contact_email_ocm',\n",
    "        'AddressInfo.RelatedURL': 'website_ocm'\n",
    "    }\n",
    "\n",
    "    df_osm.rename(columns=common_columns_osm, inplace=True)\n",
    "    df_ocm.rename(columns=common_columns_ocm, inplace=True)\n",
    "\n",
    "    # Merge DataFrames\n",
    "    merged_df = pd.concat([df_osm, df_ocm], ignore_index=True, sort=False)\n",
    "\n",
    "    # Unify common columns\n",
    "    common_columns = {\n",
    "        'latitude': ['latitude_osm', 'latitude_ocm'],\n",
    "        'longitude': ['longitude_osm', 'longitude_ocm'],\n",
    "        'amenity': ['amenity_osm', 'amenity_ocm'],\n",
    "        'brand': ['brand_osm', 'brand_ocm'],\n",
    "        'operator': ['operator_osm', 'operator_ocm'],\n",
    "        'capacity': ['capacity_osm', 'capacity_ocm'],\n",
    "        'cost': ['cost_osm', 'cost_ocm'],\n",
    "        'socket_type1': ['socket_type1_osm', 'socket_type1_ocm'],\n",
    "        'socket_type2': ['socket_type2_osm', 'socket_type2_ocm'],\n",
    "        'socket_chademo': ['socket_chademo_osm', 'socket_chademo_ocm'],\n",
    "        'address_house_number': ['address_house_number_osm', 'address_house_number_ocm'],\n",
    "        'address_street': ['address_street_osm', 'address_street_ocm'],\n",
    "        'address_city': ['address_city_osm', 'address_city_ocm'],\n",
    "        'address_country': ['address_country_osm', 'address_country_ocm'],\n",
    "        'address_postcode': ['address_postcode_osm', 'address_postcode_ocm'],\n",
    "        'address_state': ['address_state_osm', 'address_state_ocm'],\n",
    "        'website': ['website_osm', 'website_ocm'],\n",
    "        'network': ['network_osm', 'network_ocm']\n",
    "    }\n",
    "\n",
    "    for unified_col, cols in common_columns.items():\n",
    "        unify_columns(merged_df, cols[0], cols[1], unified_col)\n",
    "\n",
    "    # Round coordinates\n",
    "    merged_df['lat_round'] = merged_df['latitude'].round(3)\n",
    "    merged_df['lon_round'] = merged_df['longitude'].round(3)\n",
    "\n",
    "    # Mark duplicates\n",
    "    mark_prefer_ocm_duplicates(merged_df)\n",
    "\n",
    "    # Keep only unique entries\n",
    "    unique_merged_df = merged_df[merged_df['is_duplicate'] == False]\n",
    "    unique_merged_df.drop(columns=['is_duplicate', 'origin', 'lat_round', 'lon_round'], inplace=True)\n",
    "\n",
    "    # List of columns to keep\n",
    "    columns_to_keep = [\n",
    "        'brand', 'socket_type1', 'access', 'capacity', 'socket:tesla', 'socket_type2',\n",
    "        'socket:tesla_supercharger', 'socket:tesla_supercharger:output', 'socket:tesla_supercharger_ccs',\n",
    "        'socket:tesla_supercharger_ccs:output', 'authentication:app', 'authentication:nfc', 'socket_chademo',\n",
    "        'socket:chademo:output', 'socket:type2:output', 'socket:type2_combo', 'socket:type2_combo:output',\n",
    "        'socket:type1:output', 'authentication:none', 'output', 'charging_station:output', 'cost',\n",
    "        'payment:app', 'socket:tesla_destination', 'socket:tesla_standard', 'socket:tesla_standard:output',\n",
    "        'authentication:membership_card', 'socket:tesla_destination:output', 'payment:credit_cards',\n",
    "        'socket:type2_cable', 'payment:contactless', 'socket:type1_combo', 'socket:type1_combo:output',\n",
    "        'payment:free', 'socket:type2:power', 'socket:j1772', 'socket:type2:current', 'socket:type2:voltage',\n",
    "        'payment:tap_to_pay', 'socket:type2_cable:current', 'socket:type2_cable:output',\n",
    "        'socket:type2_cable:voltage', 'payment:via_app', 'socket:wall', 'payment:cash',\n",
    "        'payment:coins', 'payment:mastercard', 'payment:visa', 'payment:debit_cards',\n",
    "        'payment:cards', 'payment:membership_card', 'payment:qr_code', 'socket:unknown',\n",
    "        'payment:apple_pay', 'socket:ccs:output', 'ID', 'connections_ocm', 'NumberOfPoints',\n",
    "        'UsageType.IsPayAtLocation', 'UsageType.IsMembershipRequired', 'UsageType.IsAccessKeyRequired',\n",
    "        'StatusType.IsOperational', 'StatusType.IsUserSelectable', 'latitude', 'longitude',\n",
    "        'operator', 'fee_osm'\n",
    "    ]\n",
    "    df_cleaned = unique_merged_df[columns_to_keep]\n",
    "\n",
    "    # Parse connections\n",
    "    df_cleaned[['ConnectionType', 'Power', 'CurrentType']] = df_cleaned['connections_ocm'].apply(parse_connections)\n",
    "\n",
    "    # Ensure numeric conversion\n",
    "    df_cleaned['Power'] = pd.to_numeric(df_cleaned['Power'], errors='coerce')\n",
    "    df_cleaned['output'] = pd.to_numeric(df_cleaned['output'], errors='coerce')\n",
    "\n",
    "    # Merge 'Power' and 'output' columns\n",
    "    df_cleaned['Combined_Power_Output'] = df_cleaned.apply(\n",
    "        lambda row: max(filter(pd.notnull, [row['Power'], row['output']]), default=None), axis=1\n",
    "    )\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = [col for col in df_cleaned.columns if col.startswith('payment:') or col.startswith('socket:') or col.startswith('authentication:')]\n",
    "    df_cleaned.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    # Combine 'brand' and 'operator' into a single 'operator' column\n",
    "    def combine_brand_operator(row):\n",
    "        if pd.notnull(row['brand']):\n",
    "            return row['brand']\n",
    "        elif pd.notnull(row['operator']):\n",
    "            return row['operator']\n",
    "        return None\n",
    "\n",
    "    df_cleaned['operator'] = df_cleaned.apply(combine_brand_operator, axis=1)\n",
    "\n",
    "    # Ensure numeric conversion for 'capacity' and 'NumberOfPoints'\n",
    "    df_cleaned['capacity'] = pd.to_numeric(df_cleaned['capacity'], errors='coerce')\n",
    "    df_cleaned['NumberOfPoints'] = pd.to_numeric(df_cleaned['NumberOfPoints'], errors='coerce')\n",
    "\n",
    "    # Add 'capacity' to 'NumberOfPoints' and drop 'capacity'\n",
    "    df_cleaned['NumberOfPoints'] = df_cleaned['NumberOfPoints'].fillna(0) + df_cleaned['capacity'].fillna(0)\n",
    "    df_cleaned.drop(columns=['capacity'], inplace=True)\n",
    "\n",
    "    # Drop specified columns\n",
    "    columns_to_drop = ['brand', 'access', 'output', 'charging_station:output',\n",
    "                       'connections_ocm', 'ID', 'StatusType.IsUserSelectable',\n",
    "                       'fee_osm', 'Combined_Power_Output']\n",
    "    df_cleaned.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    # Replace values in socket columns only if they already have a value\n",
    "    df_cleaned['socket_type1'] = df_cleaned['socket_type1'].apply(lambda x: 'Type 1 (J1772)' if pd.notna(x) else x)\n",
    "    df_cleaned['socket_type2'] = df_cleaned['socket_type2'].apply(lambda x: 'Type 2 (Socket Only)' if pd.notna(x) else x)\n",
    "    df_cleaned['socket_chademo'] = df_cleaned['socket_chademo'].apply(lambda x: 'CHAdeMO' if pd.notna(x) else x)\n",
    "\n",
    "    # Merge connection types\n",
    "    def merge_connection_types(row):\n",
    "        if pd.notna(row['ConnectionType']):\n",
    "            return row['ConnectionType']\n",
    "        sockets = [row['socket_type1'], row['socket_type2'], row['socket_chademo']]\n",
    "        sockets = list(filter(pd.notna, sockets))\n",
    "        return ', '.join(sockets) if sockets else row['ConnectionType']\n",
    "\n",
    "    df_cleaned['ConnectionType'] = df_cleaned.apply(merge_connection_types, axis=1)\n",
    "    df_cleaned.drop(columns=['socket_type1', 'socket_type2', 'socket_chademo'], inplace=True)\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    df_cleaned.rename(columns={\n",
    "        'NumberOfPoints': 'ChargingPoints',\n",
    "        'UsageType.IsPayAtLocation': 'PayAtLocation',\n",
    "        'UsageType.IsMembershipRequired': 'MembershipRequired',\n",
    "        'UsageType.IsAccessKeyRequired': 'AccessKeyRequired',\n",
    "        'StatusType.IsOperational': 'IsOperational',\n",
    "        'latitude': 'Latitude',\n",
    "        'longitude': 'Longitude',\n",
    "        'operator': 'Operator',\n",
    "        'ConnectionType': 'ConnectionType',\n",
    "        'Power': 'PowerOutput',\n",
    "        'CurrentType': 'CurrentType',\n",
    "        'cost': 'Cost'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Fill missing 'Cost' and 'CurrentType' with 'Unknown'\n",
    "    df_cleaned['Cost'].fillna('Unknown', inplace=True)\n",
    "    df_cleaned['CurrentType'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    df_cleaned['CurrentType'].fillna('Unknown', inplace=True)\n",
    "    df_cleaned['PayAtLocation'].fillna('Unknown', inplace=True)\n",
    "    df_cleaned['MembershipRequired'].fillna('Unknown', inplace=True)\n",
    "    df_cleaned['AccessKeyRequired'].fillna('Unknown', inplace=True)\n",
    "    df_cleaned['IsOperational'].fillna('Unknown', inplace=True)\n",
    "    df_cleaned['Operator'].fillna('Unknown', inplace=True)\n",
    "    df_cleaned['ConnectionType'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    # Handle missing 'PowerOutput' by filling with np.nan\n",
    "    df_cleaned['PowerOutput'].fillna(np.nan, inplace=True)\n",
    "\n",
    "    # Convert any 0 values in 'ChargingPoints' to 1 and add a flag column\n",
    "    df_cleaned['ChargingPoints_Flag'] = df_cleaned['ChargingPoints'].apply(lambda x: 1 if x == 0 else 0)\n",
    "    df_cleaned['ChargingPoints'] = df_cleaned['ChargingPoints'].apply(lambda x: 1 if x == 0 else x)\n",
    "\n",
    "    # Ensure data types are correct\n",
    "    df_cleaned['ChargingPoints'] = df_cleaned['ChargingPoints'].astype(int)\n",
    "    df_cleaned['IsOperational'] = df_cleaned['IsOperational'].astype('category')\n",
    "    df_cleaned['ConnectionType'] = df_cleaned['ConnectionType'].astype('category')\n",
    "    df_cleaned['Operator'] = df_cleaned['Operator'].astype('category')\n",
    "    df_cleaned['CurrentType'] = df_cleaned['CurrentType'].astype('category')\n",
    "    df_cleaned['Cost'] = df_cleaned['Cost'].astype('category')\n",
    "    df_cleaned['PowerOutput'] = pd.to_numeric(df_cleaned['PowerOutput'], errors='coerce')\n",
    "    df_cleaned['IsOperational'] = df_cleaned['IsOperational'].astype('category')\n",
    "    df_cleaned['ConnectionType'] = df_cleaned['ConnectionType'].astype('category')\n",
    "    df_cleaned['Operator'] = df_cleaned['Operator'].astype('category')\n",
    "    df_cleaned['PayAtLocation'] = df_cleaned['PayAtLocation'].astype('category')\n",
    "    df_cleaned['MembershipRequired'] = df_cleaned['MembershipRequired'].astype('category')\n",
    "    df_cleaned['AccessKeyRequired'] = df_cleaned['AccessKeyRequired'].astype('category')\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# Modified main execution part\n",
    "API_KEY = \"38d18ea7-9248-46cf-860e-2a915c9b172e\"\n",
    "\n",
    "# Get cleaned data\n",
    "cleaned_df = get_cleaned_charging_station_data(API_KEY)\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"cleaned_charging_stations.csv\"\n",
    "cleaned_df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nData saved to {csv_filename}\")\n",
    "print(f\"Number of records saved: {len(cleaned_df)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nFirst few records of the saved data:\")\n",
    "print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rN0cVNhHrIoD",
    "outputId": "2b42500b-63d1-49a5-dc4e-a12dba8d264a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1712 entries, 0 to 2126\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype   \n",
      "---  ------               --------------  -----   \n",
      " 0   Cost                 1712 non-null   category\n",
      " 1   ChargingPoints       1712 non-null   int64   \n",
      " 2   PayAtLocation        1712 non-null   category\n",
      " 3   MembershipRequired   1712 non-null   category\n",
      " 4   AccessKeyRequired    1712 non-null   category\n",
      " 5   IsOperational        1712 non-null   category\n",
      " 6   Latitude             1712 non-null   float64 \n",
      " 7   Longitude            1712 non-null   float64 \n",
      " 8   Operator             1712 non-null   category\n",
      " 9   ConnectionType       1712 non-null   category\n",
      " 10  PowerOutput          1117 non-null   float64 \n",
      " 11  CurrentType          1712 non-null   category\n",
      " 12  ChargingPoints_Flag  1712 non-null   int64   \n",
      "dtypes: category(8), float64(3), int64(2)\n",
      "memory usage: 110.1 KB\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1712 records from cleaned data\n",
      "\n",
      "Enriched data saved to melbourne_enriched_charging_stations.csv\n",
      "Total records: 1712\n",
      "\n",
      "New columns added:\n",
      "- AvgBatteryCapacity\n",
      "- AvgChargingDuration\n",
      "- ClimateZone\n",
      "- DailyTrafficVolume\n",
      "- DailyUtilizationRate\n",
      "- DistanceToHighway\n",
      "- InstallationDate\n",
      "- LandUseType\n",
      "- MaxChargingSpeed\n",
      "- MedianIncome\n",
      "- MostCommonEVModel\n",
      "- NearbyRestaurants\n",
      "- NearbyShoppingCenters\n",
      "- ParkingCapacity\n",
      "- PeakHours\n",
      "- PopulationDensity\n",
      "- WeeklyPeakDay\n",
      "\n",
      "Sample of enriched data (first record):\n",
      "DailyTrafficVolume: 6072\n",
      "ParkingCapacity: 11\n",
      "MostCommonEVModel: MG ZS EV\n",
      "DailyUtilizationRate: 0.31041454815244207\n",
      "ClimateZone: Temperate Oceanic (Cfb)\n",
      "PopulationDensity: 2090.3347791444667\n",
      "AvgChargingDuration: 44.54398862887143\n",
      "NearbyRestaurants: 5\n",
      "PeakHours: 07:00-09:00, 17:00-19:00\n",
      "AvgBatteryCapacity: 72.75224032329703\n",
      "DistanceToHighway: 2.9693471795305757\n",
      "WeeklyPeakDay: Monday\n",
      "MedianIncome: 77297.18360039391\n",
      "InstallationDate: 2019-08-18 07:47:56.212741088\n",
      "LandUseType: Activity Centre\n",
      "MaxChargingSpeed: 9.037328389009419\n",
      "NearbyShoppingCenters: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from timezonefinder import TimezoneFinder\n",
    "\n",
    "class MelbourneEVDataEnricher:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"Initialize with existing Melbourne charging station DataFrame\"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.tf = TimezoneFinder()\n",
    "        \n",
    "    def add_location_based_data(self):\n",
    "        \"\"\"Add Melbourne-specific location-based enrichment data\"\"\"\n",
    "        # Melbourne population density varies by suburb (people per km²)\n",
    "        # CBD: ~20,000, Inner suburbs: 5,000-8,000, Outer suburbs: 2,000-4,000\n",
    "        self.df['PopulationDensity'] = np.where(\n",
    "            (self.df['Latitude'].between(-37.81, -37.82)) & \n",
    "            (self.df['Longitude'].between(144.95, 144.97)),\n",
    "            np.random.uniform(15000, 20000, size=len(self.df)),  # CBD\n",
    "            np.where(\n",
    "                (self.df['Latitude'].between(-37.75, -37.85)) & \n",
    "                (self.df['Longitude'].between(144.90, 145.00)),\n",
    "                np.random.uniform(5000, 8000, size=len(self.df)),  # Inner suburbs\n",
    "                np.random.uniform(2000, 4000, size=len(self.df))   # Outer suburbs\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Distance to major roads (Monash Freeway, Western Ring Road, etc.)\n",
    "        self.df['DistanceToHighway'] = np.random.uniform(0.1, 3.0, size=len(self.df))\n",
    "        \n",
    "        # Daily traffic volume based on location\n",
    "        self.df['DailyTrafficVolume'] = np.where(\n",
    "            (self.df['Latitude'].between(-37.81, -37.82)) & \n",
    "            (self.df['Longitude'].between(144.95, 144.97)),\n",
    "            np.random.randint(20000, 50000, size=len(self.df)),  # CBD\n",
    "            np.random.randint(5000, 20000, size=len(self.df))    # Other areas\n",
    "        )\n",
    "        \n",
    "        # Nearby amenities (Melbourne-specific)\n",
    "        self.df['NearbyRestaurants'] = np.where(\n",
    "            (self.df['Latitude'].between(-37.81, -37.82)) & \n",
    "            (self.df['Longitude'].between(144.95, 144.97)),\n",
    "            np.random.randint(10, 30, size=len(self.df)),  # CBD\n",
    "            np.random.randint(2, 10, size=len(self.df))    # Other areas\n",
    "        )\n",
    "        \n",
    "        # Shopping centers and parking\n",
    "        self.df['NearbyShoppingCenters'] = np.random.randint(0, 3, size=len(self.df))\n",
    "        self.df['ParkingCapacity'] = np.where(\n",
    "            (self.df['Latitude'].between(-37.81, -37.82)) & \n",
    "            (self.df['Longitude'].between(144.95, 144.97)),\n",
    "            np.random.randint(20, 100, size=len(self.df)),  # CBD\n",
    "            np.random.randint(5, 20, size=len(self.df))     # Other areas\n",
    "        )\n",
    "        \n",
    "        # Land use types (Melbourne-specific)\n",
    "        melbourne_land_uses = [\n",
    "            'Commercial', 'Residential', 'Mixed Use', \n",
    "            'Activity Centre', 'Industrial', 'Public Use'\n",
    "        ]\n",
    "        self.df['LandUseType'] = np.random.choice(melbourne_land_uses, size=len(self.df))\n",
    "        \n",
    "        # Median income by area (based on Melbourne demographics)\n",
    "        self.df['MedianIncome'] = np.where(\n",
    "            (self.df['Latitude'].between(-37.81, -37.82)) & \n",
    "            (self.df['Longitude'].between(144.95, 144.97)),\n",
    "            np.random.uniform(85000, 120000, size=len(self.df)),  # CBD and inner suburbs\n",
    "            np.random.uniform(65000, 85000, size=len(self.df))    # Other areas\n",
    "        )\n",
    "        \n",
    "        # Melbourne climate zones\n",
    "        self.df['ClimateZone'] = 'Temperate Oceanic (Cfb)'  # Melbourne's Köppen climate classification\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def add_time_usage_data(self):\n",
    "        \"\"\"Add Melbourne-specific time and usage patterns\"\"\"\n",
    "        # Melbourne peak hours\n",
    "        def generate_melbourne_peak_hours():\n",
    "            morning_start = np.random.randint(7, 9)  # Melbourne morning peak\n",
    "            evening_start = np.random.randint(16, 18)  # Melbourne evening peak\n",
    "            return f\"{morning_start:02d}:00-{morning_start+2:02d}:00, {evening_start:02d}:00-{evening_start+2:02d}:00\"\n",
    "        \n",
    "        self.df['PeakHours'] = [generate_melbourne_peak_hours() for _ in range(len(self.df))]\n",
    "        \n",
    "        # Usage patterns (adjusted for Melbourne)\n",
    "        self.df['AvgChargingDuration'] = np.random.uniform(30, 90, size=len(self.df))  # minutes\n",
    "        self.df['DailyUtilizationRate'] = np.where(\n",
    "            (self.df['Latitude'].between(-37.81, -37.82)) & \n",
    "            (self.df['Longitude'].between(144.95, 144.97)),\n",
    "            np.random.uniform(0.5, 0.9, size=len(self.df)),  # CBD\n",
    "            np.random.uniform(0.3, 0.7, size=len(self.df))   # Other areas\n",
    "        )\n",
    "        \n",
    "        # Peak days (accounting for Melbourne's business districts)\n",
    "        self.df['WeeklyPeakDay'] = np.random.choice(\n",
    "            ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n",
    "            size=len(self.df),\n",
    "            p=[0.2, 0.2, 0.25, 0.25, 0.1]  # Higher mid-week usage\n",
    "        )\n",
    "        \n",
    "        # Station age (based on Melbourne's EV infrastructure timeline)\n",
    "        installation_dates = pd.date_range(start='2019-01-01', end='2023-12-31', periods=len(self.df))\n",
    "        self.df['InstallationDate'] = np.random.choice(installation_dates, size=len(self.df))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def add_vehicle_data(self):\n",
    "        \"\"\"Add Melbourne-specific vehicle data\"\"\"\n",
    "        # Popular EV models in Melbourne/Australia\n",
    "        melbourne_ev_models = [\n",
    "            'Tesla Model 3', 'Tesla Model Y', \n",
    "            'BYD Atto 3', 'MG ZS EV', \n",
    "            'Hyundai Kona Electric', 'Polestar 2'\n",
    "        ]\n",
    "        \n",
    "        model_probabilities = [0.3, 0.2, 0.15, 0.15, 0.1, 0.1]  # Based on Australian EV sales\n",
    "        self.df['MostCommonEVModel'] = np.random.choice(\n",
    "            melbourne_ev_models, \n",
    "            size=len(self.df),\n",
    "            p=model_probabilities\n",
    "        )\n",
    "        \n",
    "        # Battery capacities based on common Australian models\n",
    "        self.df['AvgBatteryCapacity'] = np.random.uniform(60, 85, size=len(self.df))  # kWh\n",
    "        \n",
    "        # Charging speeds available in Melbourne\n",
    "        self.df['MaxChargingSpeed'] = np.where(\n",
    "            self.df['ConnectionType'] == 'CCS (Type 2)',\n",
    "            np.random.uniform(150, 250, size=len(self.df)),\n",
    "            np.where(\n",
    "                self.df['ConnectionType'] == 'CHAdeMO',\n",
    "                np.random.uniform(50, 100, size=len(self.df)),\n",
    "                np.random.uniform(7, 22, size=len(self.df))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def add_all_enrichments(self):\n",
    "        \"\"\"Add all Melbourne-specific enrichment data\"\"\"\n",
    "        return self.add_location_based_data().add_time_usage_data().add_vehicle_data()\n",
    "\n",
    "    def get_enriched_dataframe(self):\n",
    "        \"\"\"Return the enriched DataFrame\"\"\"\n",
    "        return self.df\n",
    "\n",
    "def enrich_melbourne_charging_data(input_df):\n",
    "    \"\"\"Main function to enrich Melbourne charging station data\"\"\"\n",
    "    enricher = MelbourneEVDataEnricher(input_df)\n",
    "    enriched_df = enricher.add_all_enrichments().get_enriched_dataframe()\n",
    "    return enriched_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load the cleaned data\n",
    "        original_df = pd.read_csv(\"cleaned_charging_stations.csv\")\n",
    "        print(f\"Loaded {len(original_df)} records from cleaned data\")\n",
    "        \n",
    "        # Enrich the data\n",
    "        enriched_df = enrich_melbourne_charging_data(original_df)\n",
    "        \n",
    "        # Save enriched data\n",
    "        enriched_filename = \"melbourne_enriched_charging_stations.csv\"\n",
    "        enriched_df.to_csv(enriched_filename, index=False)\n",
    "        print(f\"\\nEnriched data saved to {enriched_filename}\")\n",
    "        print(f\"Total records: {len(enriched_df)}\")\n",
    "        \n",
    "        # Display sample of new columns\n",
    "        new_columns = set(enriched_df.columns) - set(original_df.columns)\n",
    "        print(\"\\nNew columns added:\")\n",
    "        for col in sorted(new_columns):\n",
    "            print(f\"- {col}\")\n",
    "            \n",
    "        # Display sample of enriched data\n",
    "        print(\"\\nSample of enriched data (first record):\")\n",
    "        sample_record = enriched_df.iloc[0]\n",
    "        for col in new_columns:\n",
    "            print(f\"{col}: {sample_record[col]}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data enrichment process: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
